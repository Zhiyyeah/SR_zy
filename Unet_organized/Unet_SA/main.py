import os
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import numpy as np
import time
import matplotlib.pyplot as plt

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from model_attention import UNetSA # å‡è®¾è¿™äº›æ¨¡å—å­˜åœ¨ä¸”è·¯å¾„æ­£ç¡®
from data_loader import create_train_test_dataloaders
from metrics import psnr, compute_ssim
from utils import visualize_results, save_plots, save_metrics_to_file, get_device # bilinear_interpolation ä¸å†ç›´æ¥ä»è¿™é‡Œå¯¼å…¥åˆ°test_model
from model_io import save_model, load_model

# ====================== é…ç½®å‚æ•° ======================
# (é…ç½®å‚æ•°éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œæ­¤å¤„çœç•¥ä»¥ä¿æŒç®€æ´)
experiment_name = 'best_of_500_bs_256' # å¯ä»¥æ›´æ–°å®éªŒå
# æ•°æ®è®¾ç½®
lr_dir = '/public/home/zyye/SR_backup/Imagery/Water_TOA_tiles_lr'  # ä½åˆ†è¾¨ç‡å›¾åƒè·¯å¾„
hr_dir = '/public/home/zyye/SR_backup/Imagery/Water_TOA_tiles'  # é«˜åˆ†è¾¨ç‡å›¾åƒè·¯å¾„
train_ratio = 0.8

# æ¨¡å‹è®¾ç½®
up_scale = 8
width = 64

# è®­ç»ƒè®¾ç½®
batch_size = 256
num_workers = 4
pin_memory = True
seed = 42
epochs = 100
learning_rate = 0.00043
weight_decay = 0

# è®¾å¤‡è®¾ç½®
device = get_device()

# è¾“å‡ºè®¾ç½®
output_dir = './outputs'
save_interval = 1


# å¯è§†åŒ–è®¾ç½®
rgb_channels = [3, 2, 1]


# ====================== è¾…åŠ©å‡½æ•° ======================

def train_epoch(model, dataloader, criterion, optimizer):
    """è®­ç»ƒä¸€ä¸ªè½®æ¬¡"""
    model.train()
    running_loss = 0.0
    running_psnr = 0.0
    running_ssim = 0.0
    total_samples = 0

    loop = tqdm(enumerate(dataloader), total=len(dataloader), ncols=100)

    for i, data in loop:
        lr_imgs = data[0].to(device)
        hr_imgs = data[1].to(device)
        current_batch_size = lr_imgs.size(0)
        total_samples += current_batch_size

        optimizer.zero_grad()
        sr_imgs = model(lr_imgs)
        loss = criterion(sr_imgs, hr_imgs)#torch.sqrt() # RMSE æŸå¤±
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * current_batch_size
        batch_psnr = psnr(hr_imgs, sr_imgs) # é‡å‘½åé¿å…ä¸å‡½æ•°åå†²çª
        running_psnr += batch_psnr * current_batch_size
        batch_ssim = compute_ssim(hr_imgs, sr_imgs) # é‡å‘½å
        running_ssim += batch_ssim * current_batch_size

        loop.set_description(f"è®­ç»ƒæ‰¹æ¬¡ {i+1}/{len(dataloader)}")
        loop.set_postfix(loss=loss.item(), psnr=batch_psnr, ssim=batch_ssim)

    epoch_loss = running_loss / total_samples if total_samples > 0 else 0
    epoch_psnr = running_psnr / total_samples if total_samples > 0 else 0
    epoch_ssim = running_ssim / total_samples if total_samples > 0 else 0

    avg_metrics = {
        'loss': epoch_loss,
        'psnr': epoch_psnr,
        'ssim': epoch_ssim
    }

    return avg_metrics


def test_model(model, test_loader, criterion, device):
    """
    æµ‹è¯•æ¨¡å‹ï¼Œè®¡ç®—æ•´ä½“çš„ RMSE æŸå¤±ã€PSNR å’Œ SSIMï¼Œè¿”å›æ ¼å¼ä¸ train_epoch ä¸€è‡´ã€‚
    å¯é€‰åœ°å°†å¯è§†åŒ–ç»“æœä¿å­˜åˆ° results_dirã€‚
    """
    model.eval()
    running_loss = 0.0
    running_psnr = 0.0
    running_ssim = 0.0
    total_samples = 0

    print(f"æµ‹è¯•æ¨¡å‹ï¼Œå…± {len(test_loader.dataset)} å¼ å›¾åƒ...")
    with torch.no_grad():
        for i, data in enumerate(tqdm(test_loader, desc="æµ‹è¯•ä¸­", ncols=100)):
            lr_imgs, hr_imgs = data
            lr_imgs = lr_imgs.to(device)
            hr_imgs = hr_imgs.to(device)
            batch_size = lr_imgs.size(0)
            total_samples += batch_size

            # å‰å‘æ¨ç†
            sr_imgs = model(lr_imgs)

            # RMSE æŸå¤±ï¼ˆä¸ train_epoch ä¿æŒä¸€è‡´ï¼‰
            batch_loss = criterion(sr_imgs, hr_imgs)#torch.sqrt()
            running_loss += batch_loss.item() * batch_size

            # æ‰¹æ¬¡ PSNR å’Œ SSIM ï¼ˆå¯¹æ•´æ‰¹å–å¹³å‡ï¼Œå†åŠ æƒç´¯åŠ ï¼‰
            batch_psnr = psnr(hr_imgs, sr_imgs)
            running_psnr += batch_psnr * batch_size

            batch_ssim = compute_ssim(hr_imgs, sr_imgs)
            running_ssim += batch_ssim * batch_size

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    epoch_loss = running_loss / total_samples if total_samples > 0 else 0.0
    epoch_psnr = running_psnr / total_samples if total_samples > 0 else 0.0
    epoch_ssim = running_ssim / total_samples if total_samples > 0 else 0.0

    avg_metrics = {
        'loss': epoch_loss,
        'psnr': epoch_psnr,
        'ssim': epoch_ssim
    }



    return avg_metrics


def train_and_test():
    """è®­ç»ƒå¹¶æµ‹è¯•æ¨¡å‹"""

    #ä¿å­˜è·¯å¾„
    experiment_dir = os.path.join(output_dir, f"{experiment_name}_{learning_rate}")
    model_dir = os.path.join(experiment_dir, 'models')
    # æ‰€æœ‰æµ‹è¯•ç»“æœå°†ä¿å­˜åˆ°æ­¤ç›®å½•
    test_results_dir = os.path.join(experiment_dir, 'test_results')
    plots_dir = os.path.join(experiment_dir, 'plots')
    # åˆ›å»ºä¿å­˜è·¯å¾„
    for dir_path in [experiment_dir, model_dir, test_results_dir, plots_dir]:
        os.makedirs(dir_path, exist_ok=True)

    # åˆ›å»ºdataloader
    train_loader, test_loader = create_train_test_dataloaders(
        lr_dir, hr_dir, batch_size, train_ratio, seed, num_workers, pin_memory
    )
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_loader.dataset)}")
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_loader.dataset)}")

    num_channels = 7

    # åˆ›å»ºæ¨¡å‹
    model = UNetSA(up_scale=up_scale, img_channel=num_channels, width=width).to(device)
    num_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œå…± {num_params} ä¸ªå‚æ•°")

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    criterion = nn.MSELoss() # MSEæŸå¤±å‡½æ•°ï¼Œtrain_epoch å’Œ test_model å†…éƒ¨ä¼šå–sqrtå¾—åˆ°RMSE

    #è®­ç»ƒå‰å‡†å¤‡
    train_history = {'loss': [], 'psnr': [], 'ssim': []}
    test_history = {'loss': [], 'psnr': [], 'ssim': []}
    best_test_psnr = 0.0

    print(f"å¼€å§‹è®­ç»ƒï¼Œå…± {epochs} ä¸ªè½®æ¬¡...")
    total_training_start_time = time.time()

    for epoch in range(epochs):
        epoch_start_time = time.time()
        print(f"\nğŸ“˜ è½®æ¬¡ {epoch+1}/{epochs}")

        # è®­ç»ƒã€æ•°æ®ä¿å­˜
        train_metrics = train_epoch(model, train_loader, criterion, optimizer)
        for k, v in train_metrics.items():
            train_history[k].append(v)
        print(f"è®­ç»ƒæŸå¤±: {train_metrics['loss']:.4f}, è®­ç»ƒPSNR: {train_metrics['psnr']:.4f}, è®­ç»ƒSSIM: {train_metrics['ssim']:.4f}")

        # æµ‹è¯•ã€æ•°æ®ä¿å­˜
        test_metrics = test_model(model, test_loader, criterion, device)
        for k, v in test_metrics.items():
            test_history[k].append(v)
        print(f"æµ‹è¯•æŸå¤±: {test_metrics['loss']:.4f}, æµ‹è¯•PSNR: {test_metrics['psnr']:.4f}, æµ‹è¯•SSIM: {test_metrics['ssim']:.4f}")

        # ç”»æ›²çº¿å›¾
        save_plots(train_history, test_history, plots_dir)

        # ä¿å­˜æ¨¡å‹
        if (epoch + 1) % save_interval == 0:
            save_model(model, model_dir, f"epoch_{epoch+1}.pth", train_metrics)
            print(f"å·²ä¿å­˜è½®æ¬¡ {epoch+1} çš„æ¨¡å‹")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_metrics['psnr'] > best_test_psnr:
            best_test_psnr = test_metrics['psnr']
            save_model(model, model_dir, "best_model.pth", test_metrics)
            print(f"å·²ä¿å­˜æœ€ä½³æ¨¡å‹ (åŸºäºæµ‹è¯•PSNR): {best_test_psnr:.4f}")

        epoch_time_taken = time.time() - epoch_start_time
        print(f"è½®æ¬¡ {epoch+1} è€—æ—¶: {epoch_time_taken:.2f} ç§’")

    total_training_time = time.time() - total_training_start_time
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_training_time // 3600:.0f}å°æ—¶ {(total_training_time % 3600) // 60:.0f}åˆ†é’Ÿ {total_training_time % 60:.2f}ç§’")

    save_model(model, model_dir, "final_model.pth", train_metrics)
    print(f"å·²ä¿å­˜æœ€ç»ˆæ¨¡å‹")

    print("\nğŸ“Š ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æœ€ç»ˆæ¨¡å‹...")
    print("åŠ è½½æœ€ä½³æ¨¡å‹ (åŸºäºæµ‹è¯•PSNR) è¿›è¡Œæµ‹è¯•...")
    try:
        best_model_path = os.path.join(model_dir, "best_model.pth")
        if os.path.exists(best_model_path):
            # å‡è®¾ load_model ä¿®æ”¹ä¼ å…¥çš„ model å¯¹è±¡çš„çŠ¶æ€
            load_model(model, best_model_path, device)
            print(f"å·²åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_path}")
        else:
            print(f"è­¦å‘Š: best_model.pth æœªæ‰¾åˆ° ({best_model_path})ï¼Œå°†ä½¿ç”¨å½“å‰è½®æ¬¡çš„æœ€ç»ˆæ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚")
    except Exception as e:
        print(f"åŠ è½½æœ€ä½³æ¨¡å‹å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨å½“å‰è½®æ¬¡çš„æœ€ç»ˆæ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚")

    # å°† criterion å’Œ test_results_dir ä¼ é€’ç»™ test_model
    final_test_metrics = test_model(model, test_loader, criterion, device)

    return model, final_test_metrics


# ====================== ä¸»å‡½æ•° ======================
def main():
    """ä¸»å‡½æ•°"""

    print("ğŸš€ GPU å¯ç”¨:", torch.cuda.is_available())

    if torch.cuda.is_available():
        print("ğŸ§  å½“å‰ä½¿ç”¨çš„ GPU æ•°é‡:", torch.cuda.device_count())
        print("ğŸ“ å½“å‰é»˜è®¤è®¾å¤‡:", torch.cuda.current_device())
        print("ğŸ’» å½“å‰è®¾å¤‡åç§°:", torch.cuda.get_device_name(torch.cuda.current_device()))
        
        # âœ… ç®€æ´ç‰ˆå†…å­˜ä¿¡æ¯
        device = torch.cuda.current_device()
        total_gb = torch.cuda.get_device_properties(device).total_memory / (1024**3)
        used_gb = torch.cuda.memory_allocated(device) / (1024**3)
        print(f"ğŸ’¾ GPU å†…å­˜: {used_gb:.2f}/{total_gb:.2f} GB ({used_gb/total_gb*100:.1f}%)")
    else:
        print("âš ï¸ å½“å‰ä½¿ç”¨ CPU")
    print("\nğŸ“‹ é…ç½®:")
    print(f"  æ•°æ®ç›®å½•: LR='{lr_dir}', HR='{hr_dir}'")
    # ... å…¶ä»–é…ç½®æ‰“å° ...
    print(f"  è¾“å‡ºç›®å½•: {os.path.join(output_dir, f'{experiment_name}_{learning_rate}')}")
    print(f"  æµ‹è¯•ç»“æœå°†ä¿å­˜åœ¨: {os.path.join(output_dir, f'{experiment_name}_{learning_rate}', 'test_results')}")


    print("\nğŸš€ å¼€å§‹è®­ç»ƒå’Œæµ‹è¯•...")
    _, final_metrics = train_and_test() # final_metrics ç°åœ¨åŒ…å« 'loss', 'psnr', 'ssim'

    print("\nâœ… è®­ç»ƒå’Œæµ‹è¯•å®Œæˆ!")
    # final_metrics çš„é”®åå·²æ›´æ–°
    print(f"æœ€ç»ˆæµ‹è¯•é›†ç»“æœ: Loss={final_metrics['loss']:.4f}, PSNR={final_metrics['psnr']:.4f}, SSIM={final_metrics['ssim']:.4f}")


if __name__ == "__main__":
    main()