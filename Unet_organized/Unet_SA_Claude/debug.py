import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from data_loader import create_train_val_test_dataloaders, SRDataset
import rasterio

def check_data_quality():
    """æ£€æŸ¥æ•°æ®è´¨é‡å’ŒèŒƒå›´"""
    
    # é…ç½®è·¯å¾„
    lr_dir = r"D:\Py_Code\Unet_SR\SR_zy\Imagey\Imagery_WaterLand\WaterLand_TOA_tiles_lr"
    hr_dir = r"D:\Py_Code\Unet_SR\SR_zy\Imagey\Imagery_WaterLand\WaterLand_TOA_tiles_hr"
    
    print("ğŸ” å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥...")
    
    # 1. æ£€æŸ¥æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
    dataset = SRDataset(lr_dir, hr_dir)
    print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"æ€»æ ·æœ¬æ•°: {len(dataset)}")
    
    # 2. æ£€æŸ¥å‰å‡ ä¸ªæ ·æœ¬çš„åŸå§‹æ•°æ®
    print(f"\nğŸ”¬ æ£€æŸ¥å‰3ä¸ªæ ·æœ¬çš„åŸå§‹æ•°æ®:")
    for i in range(min(3, len(dataset))):
        lr_path = dataset.lr_paths[i]
        hr_path = dataset.hr_paths[i]
        
        print(f"\n--- æ ·æœ¬ {i} ---")
        print(f"LR: {os.path.basename(lr_path)}")
        print(f"HR: {os.path.basename(hr_path)}")
        
        # ç›´æ¥è¯»å–åŸå§‹æ–‡ä»¶
        with rasterio.open(lr_path) as src:
            lr_raw = src.read()
            print(f"LR åŸå§‹å½¢çŠ¶: {lr_raw.shape}")
            print(f"LR åŸå§‹æ•°æ®ç±»å‹: {lr_raw.dtype}")
            print(f"LR åŸå§‹å€¼èŒƒå›´: [{lr_raw.min():.6f}, {lr_raw.max():.6f}]")
            print(f"LR åŸå§‹å‡å€¼: {lr_raw.mean():.6f}")
            print(f"LR æ˜¯å¦æœ‰NaN: {np.isnan(lr_raw).any()}")
            print(f"LR æ˜¯å¦æœ‰Inf: {np.isinf(lr_raw).any()}")
        
        with rasterio.open(hr_path) as src:
            hr_raw = src.read()
            print(f"HR åŸå§‹å½¢çŠ¶: {hr_raw.shape}")
            print(f"HR åŸå§‹æ•°æ®ç±»å‹: {hr_raw.dtype}")
            print(f"HR åŸå§‹å€¼èŒƒå›´: [{hr_raw.min():.6f}, {hr_raw.max():.6f}]")
            print(f"HR åŸå§‹å‡å€¼: {hr_raw.mean():.6f}")
            print(f"HR æ˜¯å¦æœ‰NaN: {np.isnan(hr_raw).any()}")
            print(f"HR æ˜¯å¦æœ‰Inf: {np.isinf(hr_raw).any()}")
        
        # æ£€æŸ¥é€šè¿‡Datasetç±»åŠ è½½çš„æ•°æ®
        lr_tensor, hr_tensor = dataset[i]
        print(f"\nDatasetåŠ è½½å:")
        print(f"LR tensorå½¢çŠ¶: {lr_tensor.shape}")
        print(f"LR tensorå€¼èŒƒå›´: [{lr_tensor.min():.6f}, {lr_tensor.max():.6f}]")
        print(f"HR tensorå½¢çŠ¶: {hr_tensor.shape}")
        print(f"HR tensorå€¼èŒƒå›´: [{hr_tensor.min():.6f}, {hr_tensor.max():.6f}]")

def visualize_samples():
    """å¯è§†åŒ–æ ·æœ¬æ•°æ®"""
    
    lr_dir = r"D:\Py_Code\Unet_SR\SR_zy\Imagey\Imagery_WaterLand\WaterLand_TOA_tiles_lr"
    hr_dir = r"D:\Py_Code\Unet_SR\SR_zy\Imagey\Imagery_WaterLand\WaterLand_TOA_tiles_hr"
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = create_train_val_test_dataloaders(
        lr_dir, hr_dir, batch_size=1, 
        train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,
        seed=42, num_workers=0, pin_memory=False
    )
    
    # è·å–ä¸€ä¸ªbatch
    lr_batch, hr_batch = next(iter(train_loader))
    lr_img = lr_batch[0]  # [C, H, W]
    hr_img = hr_batch[0]  # [C, H, W]
    
    print(f"\nğŸ“¸ å¯è§†åŒ–æ ·æœ¬:")
    print(f"LRå›¾åƒå½¢çŠ¶: {lr_img.shape}")
    print(f"HRå›¾åƒå½¢çŠ¶: {hr_img.shape}")
    
    # é€‰æ‹©RGBé€šé“è¿›è¡Œå¯è§†åŒ– (å‡è®¾é€šé“3,2,1å¯¹åº”RGB)
    if lr_img.shape[0] >= 4:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„é€šé“
        # é€‰æ‹©å‰3ä¸ªé€šé“ä½œä¸ºRGB
        lr_rgb = lr_img[:3].permute(1, 2, 0).numpy()
        hr_rgb = hr_img[:3].permute(1, 2, 0).numpy()
        
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´ç”¨äºæ˜¾ç¤º
        def normalize_for_display(img):
            img_min, img_max = img.min(), img.max()
            if img_max > img_min:
                return (img - img_min) / (img_max - img_min)
            return img
        
        lr_rgb_norm = normalize_for_display(lr_rgb)
        hr_rgb_norm = normalize_for_display(hr_rgb)
        
        # åˆ›å»ºå›¾åƒæ˜¾ç¤º
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # LRå›¾åƒ
        axes[0, 0].imshow(lr_rgb_norm)
        axes[0, 0].set_title(f'LRå›¾åƒ {lr_img.shape[1]}x{lr_img.shape[2]}')
        axes[0, 0].axis('off')
        
        # HRå›¾åƒ  
        axes[0, 1].imshow(hr_rgb_norm)
        axes[0, 1].set_title(f'HRå›¾åƒ {hr_img.shape[1]}x{hr_img.shape[2]}')
        axes[0, 1].axis('off')
        
        # LRç¬¬ä¸€ä¸ªé€šé“çš„ç›´æ–¹å›¾
        axes[1, 0].hist(lr_img[0].flatten().numpy(), bins=50, alpha=0.7)
        axes[1, 0].set_title('LRç¬¬ä¸€é€šé“å€¼åˆ†å¸ƒ')
        axes[1, 0].set_xlabel('åƒç´ å€¼')
        axes[1, 0].set_ylabel('é¢‘æ¬¡')
        
        # HRç¬¬ä¸€ä¸ªé€šé“çš„ç›´æ–¹å›¾
        axes[1, 1].hist(hr_img[0].flatten().numpy(), bins=50, alpha=0.7)
        axes[1, 1].set_title('HRç¬¬ä¸€é€šé“å€¼åˆ†å¸ƒ')
        axes[1, 1].set_xlabel('åƒç´ å€¼')
        axes[1, 1].set_ylabel('é¢‘æ¬¡')
        
        plt.tight_layout()
        plt.savefig('data_visualization.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… å›¾åƒå·²ä¿å­˜ä¸º 'data_visualization.png'")

def check_model_output():
    """æ£€æŸ¥æ¨¡å‹è¾“å‡º"""
    from model_attention import UNetSA
    
    print(f"\nğŸ¤– æ£€æŸ¥æ¨¡å‹è¾“å‡º:")
    
    # åˆ›å»ºæ¨¡å‹
    model = UNetSA(up_scale=8, img_channel=7, width=64, dropout_rate=0.1)
    model.eval()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = torch.randn(1, 7, 64, 64)
    print(f"æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"æµ‹è¯•è¾“å…¥å€¼èŒƒå›´: [{test_input.min():.6f}, {test_input.max():.6f}]")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(test_input)
    
    print(f"æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"æ¨¡å‹è¾“å‡ºå€¼èŒƒå›´: [{output.min():.6f}, {output.max():.6f}]")
    print(f"æ¨¡å‹è¾“å‡ºå‡å€¼: {output.mean():.6f}")
    print(f"æ¨¡å‹è¾“å‡ºæ ‡å‡†å·®: {output.std():.6f}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
    if torch.isnan(output).any():
        print("âš ï¸ è­¦å‘Š: æ¨¡å‹è¾“å‡ºåŒ…å«NaNå€¼!")
    if torch.isinf(output).any():
        print("âš ï¸ è­¦å‘Š: æ¨¡å‹è¾“å‡ºåŒ…å«Infå€¼!")

def check_loss_function():
    """æ£€æŸ¥æŸå¤±å‡½æ•°è¡Œä¸º"""
    print(f"\nğŸ“‰ æ£€æŸ¥æŸå¤±å‡½æ•°:")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    hr_test = torch.randn(1, 7, 512, 512)
    sr_test = torch.randn(1, 7, 512, 512) 
    
    # æµ‹è¯•MSEæŸå¤±
    mse_loss = torch.nn.MSELoss()
    loss_value = mse_loss(sr_test, hr_test)
    
    print(f"HRæµ‹è¯•æ•°æ®èŒƒå›´: [{hr_test.min():.6f}, {hr_test.max():.6f}]")
    print(f"SRæµ‹è¯•æ•°æ®èŒƒå›´: [{sr_test.min():.6f}, {sr_test.max():.6f}]")
    print(f"MSEæŸå¤±å€¼: {loss_value.item():.6f}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¶…åˆ†è¾¨ç‡æ¨¡å‹è¯Šæ–­...")
    
    try:
        # 1. æ£€æŸ¥æ•°æ®è´¨é‡
        check_data_quality()
        
        # 2. å¯è§†åŒ–æ ·æœ¬
        print(f"\n" + "="*50)
        visualize_samples()
        
        # 3. æ£€æŸ¥æ¨¡å‹è¾“å‡º
        print(f"\n" + "="*50)
        check_model_output()
        
        # 4. æ£€æŸ¥æŸå¤±å‡½æ•°
        print(f"\n" + "="*50)
        check_loss_function()
        
        print(f"\nâœ… è¯Šæ–­å®Œæˆï¼è¯·æŸ¥çœ‹ä¸Šè¿°è¾“å‡ºæ¥è¯†åˆ«é—®é¢˜ã€‚")
        
    except Exception as e:
        print(f"âŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()