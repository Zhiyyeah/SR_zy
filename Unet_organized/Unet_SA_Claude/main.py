import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from tqdm import tqdm
import numpy as np
import time
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from model_attention import UNetSA
from data_loader import create_train_val_test_dataloaders
from metrics import psnr, compute_ssim
from utils import visualize_results, save_plots, save_metrics_to_file, get_device
from model_io import save_model, load_model

# ====================== é…ç½®å‚æ•° ======================
experiment_name = 'zy_first_optimized'
# æ•°æ®è®¾ç½®
lr_dir = r"D:\Py_Code\Unet_SR\SR_zy\Imagey\Imagery_WaterLand\WaterLand_TOA_tiles_lr"
hr_dir = r"D:\Py_Code\Unet_SR\SR_zy\Imagey\Imagery_WaterLand\WaterLand_TOA_tiles_hr"
train_ratio = 0.8

# æ¨¡å‹è®¾ç½®
up_scale = 8
width = 64
dropout_rate = 0.05  # Dropoutç‡ï¼ˆæ¨¡å‹å†…éƒ¨ä½¿ç”¨ï¼‰

# è®­ç»ƒè®¾ç½®
batch_size = 16
num_workers = 4
pin_memory = True
seed = 42
epochs = 100
learning_rate = 0.00043
weight_decay = 0.0001  # æ·»åŠ L2æ­£åˆ™åŒ–

# å­¦ä¹ ç‡è°ƒåº¦å™¨è®¾ç½®
lr_scheduler_type = 'ReduceLROnPlateau'  # å¯é€‰: 'ReduceLROnPlateau', 'CosineAnnealing'
lr_patience = 5  # ReduceLROnPlateauçš„patience
lr_factor = 0.5  # ReduceLROnPlateauçš„factor
lr_min = 1e-6  # æœ€å°å­¦ä¹ ç‡

# è®¾å¤‡è®¾ç½®
device = get_device()

# è¾“å‡ºè®¾ç½®
output_dir = './outputs'
save_interval = 5

# å¯è§†åŒ–è®¾ç½®
rgb_channels = [3, 2, 1]

# ====================== è¾…åŠ©å‡½æ•° ======================

def save_training_history(history_df, save_path):
    """ä¿å­˜è®­ç»ƒå†å²ä¸ºCSVæ–‡ä»¶"""
    history_df.to_csv(save_path, index=False)
    print(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {save_path}")

def train_epoch(model, dataloader, criterion, optimizer):
    """è®­ç»ƒä¸€ä¸ªè½®æ¬¡"""
    model.train()
    running_loss = 0.0
    running_psnr = 0.0
    running_ssim = 0.0
    total_samples = 0

    loop = tqdm(enumerate(dataloader), total=len(dataloader), ncols=100)

    for i, data in loop:
        lr_imgs = data[0].to(device)
        hr_imgs = data[1].to(device)
        current_batch_size = lr_imgs.size(0)
        total_samples += current_batch_size

        optimizer.zero_grad()
        sr_imgs = model(lr_imgs)
        loss = criterion(sr_imgs, hr_imgs)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * current_batch_size
        batch_psnr = psnr(hr_imgs, sr_imgs)
        running_psnr += batch_psnr * current_batch_size
        batch_ssim = compute_ssim(hr_imgs, sr_imgs)
        running_ssim += batch_ssim * current_batch_size

        loop.set_description(f"è®­ç»ƒæ‰¹æ¬¡ {i+1}/{len(dataloader)}")
        loop.set_postfix(loss=loss.item(), psnr=batch_psnr, ssim=batch_ssim)

    epoch_loss = running_loss / total_samples if total_samples > 0 else 0
    epoch_psnr = running_psnr / total_samples if total_samples > 0 else 0
    epoch_ssim = running_ssim / total_samples if total_samples > 0 else 0

    avg_metrics = {
        'loss': epoch_loss,
        'psnr': epoch_psnr,
        'ssim': epoch_ssim
    }

    return avg_metrics

def validate_model(model, val_loader, criterion, device):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    running_psnr = 0.0
    running_ssim = 0.0
    total_samples = 0

    with torch.no_grad():
        for i, data in enumerate(tqdm(val_loader, desc="éªŒè¯ä¸­", ncols=100)):
            lr_imgs, hr_imgs = data
            lr_imgs = lr_imgs.to(device)
            hr_imgs = hr_imgs.to(device)
            batch_size = lr_imgs.size(0)
            total_samples += batch_size

            sr_imgs = model(lr_imgs)
            batch_loss = criterion(sr_imgs, hr_imgs)
            running_loss += batch_loss.item() * batch_size

            batch_psnr = psnr(hr_imgs, sr_imgs)
            running_psnr += batch_psnr * batch_size

            batch_ssim = compute_ssim(hr_imgs, sr_imgs)
            running_ssim += batch_ssim * batch_size

    epoch_loss = running_loss / total_samples if total_samples > 0 else 0.0
    epoch_psnr = running_psnr / total_samples if total_samples > 0 else 0.0
    epoch_ssim = running_ssim / total_samples if total_samples > 0 else 0.0

    avg_metrics = {
        'loss': epoch_loss,
        'psnr': epoch_psnr,
        'ssim': epoch_ssim
    }

    return avg_metrics

def test_model(model, test_loader, criterion, device):
    """æµ‹è¯•æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    running_psnr = 0.0
    running_ssim = 0.0
    total_samples = 0

    print(f"æµ‹è¯•æ¨¡å‹ï¼Œå…± {len(test_loader.dataset)} å¼ å›¾åƒ...")
    with torch.no_grad():
        for i, data in enumerate(tqdm(test_loader, desc="æµ‹è¯•ä¸­", ncols=100)):
            lr_imgs, hr_imgs = data
            lr_imgs = lr_imgs.to(device)
            hr_imgs = hr_imgs.to(device)
            batch_size = lr_imgs.size(0)
            total_samples += batch_size

            sr_imgs = model(lr_imgs)
            batch_loss = criterion(sr_imgs, hr_imgs)
            running_loss += batch_loss.item() * batch_size

            batch_psnr = psnr(hr_imgs, sr_imgs)
            running_psnr += batch_psnr * batch_size

            batch_ssim = compute_ssim(hr_imgs, sr_imgs)
            running_ssim += batch_ssim * batch_size

    epoch_loss = running_loss / total_samples if total_samples > 0 else 0.0
    epoch_psnr = running_psnr / total_samples if total_samples > 0 else 0.0
    epoch_ssim = running_ssim / total_samples if total_samples > 0 else 0.0

    avg_metrics = {
        'loss': epoch_loss,
        'psnr': epoch_psnr,
        'ssim': epoch_ssim
    }

    return avg_metrics

def plot_learning_curves(history_df, plots_dir):
    """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # Lossæ›²çº¿
    axes[0, 0].plot(history_df['epoch'], history_df['train_loss'], label='Train Loss')
    axes[0, 0].plot(history_df['epoch'], history_df['val_loss'], label='Val Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training and Validation Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # PSNRæ›²çº¿
    axes[0, 1].plot(history_df['epoch'], history_df['train_psnr'], label='Train PSNR')
    axes[0, 1].plot(history_df['epoch'], history_df['val_psnr'], label='Val PSNR')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('PSNR (dB)')
    axes[0, 1].set_title('Training and Validation PSNR')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # SSIMæ›²çº¿
    axes[1, 0].plot(history_df['epoch'], history_df['train_ssim'], label='Train SSIM')
    axes[1, 0].plot(history_df['epoch'], history_df['val_ssim'], label='Val SSIM')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('SSIM')
    axes[1, 0].set_title('Training and Validation SSIM')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # å­¦ä¹ ç‡æ›²çº¿
    axes[1, 1].plot(history_df['epoch'], history_df['learning_rate'], label='Learning Rate')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Learning Rate')
    axes[1, 1].set_title('Learning Rate Schedule')
    axes[1, 1].set_yscale('log')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'training_curves.png'), dpi=300, bbox_inches='tight')
    plt.close()

def train_and_test():
    """è®­ç»ƒå¹¶æµ‹è¯•æ¨¡å‹"""
    
    # ä¿å­˜è·¯å¾„
    experiment_dir = os.path.join(output_dir, f"{experiment_name}_{learning_rate}")
    model_dir = os.path.join(experiment_dir, 'models')
    test_results_dir = os.path.join(experiment_dir, 'test_results')
    plots_dir = os.path.join(experiment_dir, 'plots')
    
    # åˆ›å»ºä¿å­˜è·¯å¾„
    for dir_path in [experiment_dir, model_dir, test_results_dir, plots_dir]:
        os.makedirs(dir_path, exist_ok=True)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = create_train_val_test_dataloaders(
        lr_dir, hr_dir, batch_size, train_ratio, seed, num_workers, pin_memory
    )
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_loader.dataset)}")
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_loader.dataset)}")
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_loader.dataset)}")
    
    num_channels = 7
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆæ¨¡å‹å†…éƒ¨å·²å®ç°Dropoutï¼‰
    model = UNetSA(up_scale=up_scale, img_channel=num_channels, width=width, 
                   use_attention=True, dropout_rate=dropout_rate).to(device)
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œå…± {num_params} ä¸ªå‚æ•°")
    print(f"æ¨¡å‹å†…éƒ¨Dropoutç‡: {dropout_rate}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆæ·»åŠ æƒé‡è¡°å‡ï¼‰
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    criterion = nn.MSELoss()
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    if lr_scheduler_type == 'ReduceLROnPlateau':
        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=lr_factor, 
                                    patience=lr_patience, min_lr=lr_min, verbose=True)
    elif lr_scheduler_type == 'CosineAnnealing':
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr_min)
    
    # è®­ç»ƒå†å²è®°å½•
    history_data = []
    best_val_psnr = 0.0
    
    print(f"\nå¼€å§‹è®­ç»ƒï¼Œå…± {epochs} ä¸ªè½®æ¬¡...")
    print(f"ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨: {lr_scheduler_type}")
    
    total_training_start_time = time.time()
    
    for epoch in range(epochs):
        epoch_start_time = time.time()
        current_lr = optimizer.param_groups[0]['lr']
        print(f"\nğŸ“˜ è½®æ¬¡ {epoch+1}/{epochs} (å­¦ä¹ ç‡: {current_lr:.2e})")
        
        # è®­ç»ƒ
        train_metrics = train_epoch(model, train_loader, criterion, optimizer)
        print(f"è®­ç»ƒ - Loss: {train_metrics['loss']:.4f}, PSNR: {train_metrics['psnr']:.4f}, SSIM: {train_metrics['ssim']:.4f}")
        
        # éªŒè¯
        val_metrics = validate_model(model, val_loader, criterion, device)
        print(f"éªŒè¯ - Loss: {val_metrics['loss']:.4f}, PSNR: {val_metrics['psnr']:.4f}, SSIM: {val_metrics['ssim']:.4f}")
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if lr_scheduler_type == 'ReduceLROnPlateau':
            scheduler.step(val_metrics['psnr'])
        elif lr_scheduler_type == 'CosineAnnealing':
            scheduler.step()
        
        # è®°å½•å†å²
        epoch_data = {
            'epoch': epoch + 1,
            'learning_rate': current_lr,
            'train_loss': train_metrics['loss'],
            'train_psnr': train_metrics['psnr'],
            'train_ssim': train_metrics['ssim'],
            'val_loss': val_metrics['loss'],
            'val_psnr': val_metrics['psnr'],
            'val_ssim': val_metrics['ssim'],
            'epoch_time': time.time() - epoch_start_time,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        history_data.append(epoch_data)
        
        # ä¿å­˜å†å²æ•°æ®ä¸ºCSV
        history_df = pd.DataFrame(history_data)
        history_df.to_csv(os.path.join(experiment_dir, 'training_history.csv'), index=False)
        
        # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
        if len(history_data) > 1:
            plot_learning_curves(history_df, plots_dir)
        
        # ä¿å­˜æ¨¡å‹
        if (epoch + 1) % save_interval == 0:
            save_model(model, model_dir, f"epoch_{epoch+1}.pth", val_metrics)
            print(f"å·²ä¿å­˜è½®æ¬¡ {epoch+1} çš„æ¨¡å‹")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›†PSNRï¼‰
        if val_metrics['psnr'] > best_val_psnr:
            best_val_psnr = val_metrics['psnr']
            save_model(model, model_dir, "best_model.pth", val_metrics)
            print(f"å·²ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯PSNR: {best_val_psnr:.4f})")
        
        print(f"è½®æ¬¡ {epoch+1} è€—æ—¶: {epoch_data['epoch_time']:.2f} ç§’")
    
    total_training_time = time.time() - total_training_start_time
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_training_time // 3600:.0f}å°æ—¶ {(total_training_time % 3600) // 60:.0f}åˆ†é’Ÿ {total_training_time % 60:.2f}ç§’")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_model(model, model_dir, "final_model.pth", val_metrics)
    print(f"å·²ä¿å­˜æœ€ç»ˆæ¨¡å‹")
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_data = {
        'experiment_name': experiment_name,
        'up_scale': up_scale,
        'width': width,
        'dropout_rate': dropout_rate,
        'batch_size': batch_size,
        'epochs': epoch + 1,  # å®é™…è®­ç»ƒçš„è½®æ¬¡æ•°
        'initial_learning_rate': learning_rate,
        'weight_decay': weight_decay,
        'lr_scheduler_type': lr_scheduler_type,
        'best_val_psnr': best_val_psnr,
        'total_training_time': total_training_time,
        'num_parameters': num_params
    }
    config_df = pd.DataFrame([config_data])
    config_df.to_csv(os.path.join(experiment_dir, 'training_config.csv'), index=False)
    
    # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    print("\nğŸ“Š ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    try:
        best_model_path = os.path.join(model_dir, "best_model.pth")
        if os.path.exists(best_model_path):
            load_model(model, best_model_path, device)
            print(f"å·²åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_path}")
        else:
            print(f"è­¦å‘Š: best_model.pth æœªæ‰¾åˆ°ï¼Œä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚")
    except Exception as e:
        print(f"åŠ è½½æœ€ä½³æ¨¡å‹å¤±è´¥: {e}ã€‚ä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚")
    
    # æœ€ç»ˆæµ‹è¯•
    final_test_metrics = test_model(model, test_loader, criterion, device)
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    test_results = {
        'test_loss': final_test_metrics['loss'],
        'test_psnr': final_test_metrics['psnr'],
        'test_ssim': final_test_metrics['ssim'],
        'test_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    test_df = pd.DataFrame([test_results])
    test_df.to_csv(os.path.join(experiment_dir, 'test_results.csv'), index=False)
    
    # åˆ›å»ºå¹¶ä¿å­˜å®Œæ•´çš„è®­ç»ƒæ€»ç»“
    summary_data = {
        'å®éªŒåç§°': experiment_name,
        'æ€»è½®æ¬¡': epoch + 1,
        'æœ€ä½³éªŒè¯PSNR': best_val_psnr,
        'æœ€ç»ˆæµ‹è¯•Loss': final_test_metrics['loss'],
        'æœ€ç»ˆæµ‹è¯•PSNR': final_test_metrics['psnr'],
        'æœ€ç»ˆæµ‹è¯•SSIM': final_test_metrics['ssim'],
        'æ€»è®­ç»ƒæ—¶é—´(å°æ—¶)': total_training_time / 3600,
        'æ¨¡å‹å‚æ•°é‡': num_params,
        'åˆå§‹å­¦ä¹ ç‡': learning_rate,
        'æƒé‡è¡°å‡': weight_decay,
        'Dropoutç‡': dropout_rate,
        'æ‰¹æ¬¡å¤§å°': batch_size,
        'ä¸Šé‡‡æ ·å€æ•°': up_scale,
        'æ¨¡å‹å®½åº¦': width
    }
    summary_df = pd.DataFrame([summary_data])
    summary_df.to_csv(os.path.join(experiment_dir, 'training_summary.csv'), index=False)
    print(f"\nè®­ç»ƒæ€»ç»“å·²ä¿å­˜åˆ°: {os.path.join(experiment_dir, 'training_summary.csv')}")
    
    return model, final_test_metrics

# ====================== ä¸»å‡½æ•° ======================
def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ GPU å¯ç”¨:", torch.cuda.is_available())
    
    if torch.cuda.is_available():
        print("ğŸ§  å½“å‰ä½¿ç”¨çš„ GPU æ•°é‡:", torch.cuda.device_count())
        print("ğŸ“ å½“å‰é»˜è®¤è®¾å¤‡:", torch.cuda.current_device())
        print("ğŸ’» å½“å‰è®¾å¤‡åç§°:", torch.cuda.get_device_name(torch.cuda.current_device()))
        
        device = torch.cuda.current_device()
        total_gb = torch.cuda.get_device_properties(device).total_memory / (1024**3)
        used_gb = torch.cuda.memory_allocated(device) / (1024**3)
        print(f"ğŸ’¾ GPU å†…å­˜: {used_gb:.2f}/{total_gb:.2f} GB ({used_gb/total_gb*100:.1f}%)")
    else:
        print("âš ï¸ å½“å‰ä½¿ç”¨ CPU")
    
    print("\nğŸ“‹ é…ç½®:")
    print(f"  å®éªŒåç§°: {experiment_name}")
    print(f"  æ•°æ®ç›®å½•: LR='{lr_dir}', HR='{hr_dir}'")
    print(f"  æ¨¡å‹å‚æ•°: up_scale={up_scale}, width={width}, dropout_rate={dropout_rate}")
    print(f"  è®­ç»ƒå‚æ•°: batch_size={batch_size}, epochs={epochs}, lr={learning_rate}, weight_decay={weight_decay}")
    print(f"  å­¦ä¹ ç‡è°ƒåº¦: {lr_scheduler_type}")
    print(f"  è¾“å‡ºç›®å½•: {os.path.join(output_dir, f'{experiment_name}_{learning_rate}')}")
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒå’Œæµ‹è¯•...")
    _, final_metrics = train_and_test()
    
    print("\nâœ… è®­ç»ƒå’Œæµ‹è¯•å®Œæˆ!")
    print(f"æœ€ç»ˆæµ‹è¯•é›†ç»“æœ: Loss={final_metrics['loss']:.4f}, PSNR={final_metrics['psnr']:.4f}, SSIM={final_metrics['ssim']:.4f}")

if __name__ == "__main__":
    main()