import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from tqdm import tqdm
import numpy as np
import time
import matplotlib.pyplot as plt
from collections import deque

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from model_attention import UNetSA  # ä½¿ç”¨æ”¹è¿›åçš„æ¨¡å‹
from data_loader import create_train_test_dataloaders
from metrics import psnr, compute_ssim
from utils import visualize_results, save_plots, save_metrics_to_file, get_device
from model_io import save_model, load_model

# ====================== é…ç½®å‚æ•° ======================
experiment_name = 'improved_training_no_overfit_2'  # æ›´æ–°å®éªŒå
# æ•°æ®è®¾ç½®
lr_dir = '/public/home/zyye/SR_backup/Imagery/Water_TOA_tiles_lr'  # ä½åˆ†è¾¨ç‡å›¾åƒè·¯å¾„
hr_dir = '/public/home/zyye/SR_backup/Imagery/Water_TOA_tiles'  # é«˜åˆ†è¾¨ç‡å›¾åƒè·¯å¾„
train_ratio = 0.8

# æ¨¡å‹è®¾ç½®
up_scale = 8
width = 32  # é™ä½æ¨¡å‹å®¹é‡
dropout_rate = 0.1  # æ·»åŠ dropoutç‡

# è®­ç»ƒè®¾ç½®
batch_size = 32  # é€‚å½“å‡å°batch sizeå¯ä»¥å¢åŠ å™ªå£°ï¼Œæœ‰åŠ©äºæ­£åˆ™åŒ–
num_workers = 4
pin_memory = True
seed = 42
epochs = 200  # å¢åŠ epochsï¼Œå› ä¸ºä¼šä½¿ç”¨æ—©åœ
learning_rate = 3e-4  # é€‚ä¸­çš„åˆå§‹å­¦ä¹ ç‡
weight_decay = 1e-5  # æ›´å°çš„æƒé‡è¡°å‡

# æ—©åœè®¾ç½®
early_stopping_patience = 50  # å¤§å¹…å¢åŠ patienceï¼Œç»™æ¨¡å‹å……åˆ†çš„è®­ç»ƒæ—¶é—´
min_delta = 0  # è®¾ä¸º0ï¼Œåªè¦æœ‰æ”¹å–„å°±ç»§ç»­
use_early_stopping = True  # å¯ä»¥é€‰æ‹©æ˜¯å¦ä½¿ç”¨æ—©åœ

# å­¦ä¹ ç‡è°ƒåº¦å™¨è®¾ç½®
lr_scheduler_type = 'cosine_warmup'  # ä½¿ç”¨å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«
lr_patience = 20  # å¤§å¹…å¢åŠ patience
lr_factor = 0.8  # æ›´æ¸©å’Œçš„è¡°å‡å› å­ï¼ˆä»0.5æ”¹ä¸º0.8ï¼‰
lr_min = 5e-5  # æé«˜æœ€å°å­¦ä¹ ç‡ï¼ˆä»1e-6æ”¹ä¸º5e-5ï¼‰
warmup_epochs = 5  # æ·»åŠ é¢„çƒ­æœŸ

# æ¢¯åº¦è£å‰ª
gradient_clip_norm = 1.0  # æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°

# è®¾å¤‡è®¾ç½®
device = get_device()

# è¾“å‡ºè®¾ç½®
output_dir = './outputs'
save_interval = 5  # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡

# å¯è§†åŒ–è®¾ç½®
rgb_channels = [3, 2, 1]


# ====================== æ—©åœç±» ======================
class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience=10, min_delta=0, mode='min'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        
    def __call__(self, score):
        if self.mode == 'min':
            score = -score
            
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0
            
        return self.early_stop


# ====================== æ•°æ®å¢å¼ºï¼ˆå¦‚æœdata_loaderä¸­æ²¡æœ‰ï¼‰ ======================
def get_augmentation_transforms():
    """è·å–æ•°æ®å¢å¼ºå˜æ¢"""
    import torchvision.transforms as transforms
    
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ çš„æ•°æ®æ˜¯å¤šé€šé“é¥æ„Ÿå›¾åƒ
    # å¦‚æœdata_loaderå·²ç»åŒ…å«å¢å¼ºï¼Œå¯ä»¥è·³è¿‡è¿™éƒ¨åˆ†
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        # å¯¹äºå¤šé€šé“å›¾åƒï¼Œå¯èƒ½éœ€è¦è‡ªå®šä¹‰æ—‹è½¬å’Œé¢œè‰²å¢å¼º
    ])
    
    test_transform = None  # æµ‹è¯•æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼º
    
    return train_transform, test_transform


# ====================== è¾…åŠ©å‡½æ•° ======================
def train_epoch(model, dataloader, criterion, optimizer, gradient_clip_norm=None):
    """è®­ç»ƒä¸€ä¸ªè½®æ¬¡"""
    model.train()
    running_loss = 0.0
    running_psnr = 0.0
    running_ssim = 0.0
    total_samples = 0

    loop = tqdm(enumerate(dataloader), total=len(dataloader), ncols=100)

    for i, data in loop:
        lr_imgs = data[0].to(device)
        hr_imgs = data[1].to(device)
        current_batch_size = lr_imgs.size(0)
        total_samples += current_batch_size

        optimizer.zero_grad()
        sr_imgs = model(lr_imgs)
        loss = criterion(sr_imgs, hr_imgs)
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        if gradient_clip_norm is not None:
            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_norm)
        
        optimizer.step()

        running_loss += loss.item() * current_batch_size
        batch_psnr = psnr(hr_imgs, sr_imgs)
        running_psnr += batch_psnr * current_batch_size
        batch_ssim = compute_ssim(hr_imgs, sr_imgs)
        running_ssim += batch_ssim * current_batch_size

        loop.set_description(f"è®­ç»ƒæ‰¹æ¬¡ {i+1}/{len(dataloader)}")
        loop.set_postfix(loss=loss.item(), psnr=batch_psnr, ssim=batch_ssim)

    epoch_loss = running_loss / total_samples if total_samples > 0 else 0
    epoch_psnr = running_psnr / total_samples if total_samples > 0 else 0
    epoch_ssim = running_ssim / total_samples if total_samples > 0 else 0

    avg_metrics = {
        'loss': epoch_loss,
        'psnr': epoch_psnr,
        'ssim': epoch_ssim
    }

    return avg_metrics


def test_model(model, test_loader, criterion, device):
    """æµ‹è¯•æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    running_psnr = 0.0
    running_ssim = 0.0
    total_samples = 0

    print(f"æµ‹è¯•æ¨¡å‹ï¼Œå…± {len(test_loader.dataset)} å¼ å›¾åƒ...")
    with torch.no_grad():
        for i, data in enumerate(tqdm(test_loader, desc="æµ‹è¯•ä¸­", ncols=100)):
            lr_imgs, hr_imgs = data
            lr_imgs = lr_imgs.to(device)
            hr_imgs = hr_imgs.to(device)
            batch_size = lr_imgs.size(0)
            total_samples += batch_size

            # å‰å‘æ¨ç†
            sr_imgs = model(lr_imgs)

            # æŸå¤±è®¡ç®—
            batch_loss = criterion(sr_imgs, hr_imgs)
            running_loss += batch_loss.item() * batch_size

            # æŒ‡æ ‡è®¡ç®—
            batch_psnr = psnr(hr_imgs, sr_imgs)
            running_psnr += batch_psnr * batch_size

            batch_ssim = compute_ssim(hr_imgs, sr_imgs)
            running_ssim += batch_ssim * batch_size

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    epoch_loss = running_loss / total_samples if total_samples > 0 else 0.0
    epoch_psnr = running_psnr / total_samples if total_samples > 0 else 0.0
    epoch_ssim = running_ssim / total_samples if total_samples > 0 else 0.0

    avg_metrics = {
        'loss': epoch_loss,
        'psnr': epoch_psnr,
        'ssim': epoch_ssim
    }

    return avg_metrics


def train_and_test():
    """è®­ç»ƒå¹¶æµ‹è¯•æ¨¡å‹"""

    # ä¿å­˜è·¯å¾„
    experiment_dir = os.path.join(output_dir, f"{experiment_name}_lr{learning_rate}_wd{weight_decay}")
    model_dir = os.path.join(experiment_dir, 'models')
    test_results_dir = os.path.join(experiment_dir, 'test_results')
    plots_dir = os.path.join(experiment_dir, 'plots')
    
    # åˆ›å»ºä¿å­˜è·¯å¾„
    for dir_path in [experiment_dir, model_dir, test_results_dir, plots_dir]:
        os.makedirs(dir_path, exist_ok=True)

    # ä¿å­˜é…ç½®
    config = {
        'experiment_name': experiment_name,
        'up_scale': up_scale,
        'width': width,
        'dropout_rate': dropout_rate,
        'batch_size': batch_size,
        'learning_rate': learning_rate,
        'weight_decay': weight_decay,
        'epochs': epochs,
        'early_stopping_patience': early_stopping_patience,
        'gradient_clip_norm': gradient_clip_norm,
        'lr_scheduler_type': lr_scheduler_type
    }
    
    import json
    with open(os.path.join(experiment_dir, 'config.json'), 'w') as f:
        json.dump(config, f, indent=4)

    # åˆ›å»ºdataloader
    train_loader, test_loader = create_train_test_dataloaders(
        lr_dir, hr_dir, batch_size, train_ratio, seed, num_workers, pin_memory
    )
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_loader.dataset)}")
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_loader.dataset)}")

    num_channels = 7

    # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨æ”¹è¿›çš„æ¨¡å‹ï¼‰
    model = UNetSA(
        up_scale=up_scale, 
        img_channel=num_channels, 
        width=width,
        dropout_rate=dropout_rate,
        use_attention=True
    ).to(device)
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œå…± {num_params:,} ä¸ªå‚æ•°")

    # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨AdamWï¼‰
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    
    # æŸå¤±å‡½æ•° - ä½¿ç”¨æ··åˆæŸå¤±
    l1_criterion = nn.L1Loss()
    mse_criterion = nn.MSELoss()
    
    # å®šä¹‰æ··åˆæŸå¤±å‡½æ•°
    def mixed_loss(pred, target):
        return 0.7 * l1_criterion(pred, target) + 0.3 * mse_criterion(pred, target)
    
    criterion = mixed_loss
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - æ·»åŠ æ›´å¤šé€‰é¡¹
    if lr_scheduler_type == 'reduce_on_plateau':
        scheduler = ReduceLROnPlateau(
            optimizer, 
            mode='max',  # æ”¹ä¸ºmaxï¼Œå› ä¸ºæˆ‘ä»¬è¦æœ€å¤§åŒ–PSNR
            factor=lr_factor, 
            patience=lr_patience,
            min_lr=lr_min,
            verbose=True,
            threshold=0.01  # åªæœ‰æ”¹å–„è¶…è¿‡0.01æ‰ç®—æœ‰æ•ˆæ”¹å–„
        )
    elif lr_scheduler_type == 'cosine':
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=epochs,
            eta_min=lr_min
        )
    elif lr_scheduler_type == 'cosine_warmup':
        # è‡ªå®šä¹‰å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«
        def lr_lambda(epoch):
            if epoch < warmup_epochs:
                return epoch / warmup_epochs
            else:
                progress = (epoch - warmup_epochs) / (epochs - warmup_epochs)
                return lr_min / learning_rate + (1 - lr_min / learning_rate) * 0.5 * (1 + np.cos(np.pi * progress))
        
        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    else:  # 'none' - ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦
        scheduler = None
    
    # æ—©åœæœºåˆ¶
    early_stopping = EarlyStopping(patience=early_stopping_patience, min_delta=min_delta, mode='min')

    # è®­ç»ƒå†å²
    train_history = {'loss': [], 'psnr': [], 'ssim': []}
    test_history = {'loss': [], 'psnr': [], 'ssim': []}
    best_test_psnr = 0.0
    best_test_loss = float('inf')

    print(f"å¼€å§‹è®­ç»ƒï¼Œå…± {epochs} ä¸ªè½®æ¬¡...")
    print(f"ä½¿ç”¨æ—©åœæœºåˆ¶ï¼Œpatience={early_stopping_patience}")
    print(f"ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨: {lr_scheduler_type}")
    print(f"ä½¿ç”¨æ¢¯åº¦è£å‰ª: max_norm={gradient_clip_norm}")
    
    total_training_start_time = time.time()

    for epoch in range(epochs):
        epoch_start_time = time.time()
        print(f"\nğŸ“˜ è½®æ¬¡ {epoch+1}/{epochs}")
        print(f"å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")

        # è®­ç»ƒ
        train_metrics = train_epoch(model, train_loader, criterion, optimizer, gradient_clip_norm)
        for k, v in train_metrics.items():
            train_history[k].append(v)
        print(f"è®­ç»ƒ - Loss: {train_metrics['loss']:.4f}, PSNR: {train_metrics['psnr']:.2f} dB, SSIM: {train_metrics['ssim']:.4f}")

        # æµ‹è¯•
        test_metrics = test_model(model, test_loader, criterion, device)
        for k, v in test_metrics.items():
            test_history[k].append(v)
        print(f"æµ‹è¯• - Loss: {test_metrics['loss']:.4f}, PSNR: {test_metrics['psnr']:.2f} dB, SSIM: {test_metrics['ssim']:.4f}")

        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler is not None:
            if lr_scheduler_type == 'reduce_on_plateau':
                scheduler.step(test_metrics['psnr'])  # åŸºäºPSNRè°ƒæ•´
            else:
                scheduler.step()

        # æ—©åœæ£€æŸ¥ - åŸºäºPSNRè€Œä¸æ˜¯æŸå¤±
        if use_early_stopping and early_stopping(-test_metrics['psnr']):  # è´Ÿå·å› ä¸ºæˆ‘ä»¬è¦æœ€å¤§åŒ–PSNR
            print(f"\næ—©åœè§¦å‘ï¼æµ‹è¯•PSNRåœ¨ {early_stopping_patience} ä¸ªepochså†…æ²¡æœ‰æ”¹å–„ã€‚")
            break

        # ä¿å­˜å›¾è¡¨
        save_plots(train_history, test_history, plots_dir)

        # å®šæœŸä¿å­˜æ¨¡å‹
        if (epoch + 1) % save_interval == 0:
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_metrics': train_metrics,
                'test_metrics': test_metrics
            }
            torch.save(checkpoint, os.path.join(model_dir, f"checkpoint_epoch_{epoch+1}.pth"))
            print(f"å·²ä¿å­˜æ£€æŸ¥ç‚¹ (epoch {epoch+1})")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_metrics['loss'] < best_test_loss:
            best_test_loss = test_metrics['loss']
            best_test_psnr = test_metrics['psnr']
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_metrics': train_metrics,
                'test_metrics': test_metrics
            }
            torch.save(checkpoint, os.path.join(model_dir, "best_model.pth"))
            print(f"âœ¨ ä¿å­˜æœ€ä½³æ¨¡å‹ - Loss: {best_test_loss:.4f}, PSNR: {best_test_psnr:.2f} dB")

        epoch_time = time.time() - epoch_start_time
        print(f"è½®æ¬¡è€—æ—¶: {epoch_time:.2f} ç§’")

    total_training_time = time.time() - total_training_start_time
    hours = int(total_training_time // 3600)
    minutes = int((total_training_time % 3600) // 60)
    seconds = total_training_time % 60
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {hours}å°æ—¶ {minutes}åˆ†é’Ÿ {seconds:.2f}ç§’")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_checkpoint = {
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'train_history': train_history,
        'test_history': test_history
    }
    torch.save(final_checkpoint, os.path.join(model_dir, "final_model.pth"))

    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    print("\nğŸ“Š åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
    best_checkpoint = torch.load(os.path.join(model_dir, "best_model.pth"))
    model.load_state_dict(best_checkpoint['model_state_dict'])
    
    final_test_metrics = test_model(model, test_loader, criterion, device)

    # ä¿å­˜è®­ç»ƒæ€»ç»“
    summary = {
        'total_epochs': epoch + 1,
        'best_epoch': best_checkpoint['epoch'],
        'best_test_metrics': best_checkpoint['test_metrics'],
        'final_test_metrics': final_test_metrics,
        'total_training_time': total_training_time,
        'model_parameters': num_params
    }
    
    with open(os.path.join(experiment_dir, 'training_summary.json'), 'w') as f:
        json.dump(summary, f, indent=4)

    return model, final_test_metrics


# ====================== ä¸»å‡½æ•° ======================+
def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("è¶…åˆ†è¾¨ç‡æ¨¡å‹è®­ç»ƒ - æ”¹è¿›ç‰ˆ")
    print("="*60)
    
    print("\nğŸš€ GPU ä¿¡æ¯:")
    if torch.cuda.is_available():
        print(f"  âœ… GPU å¯ç”¨")
        print(f"  ğŸ“ è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        print(f"  ğŸ’» å½“å‰è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        
        device = torch.cuda.current_device()
        total_gb = torch.cuda.get_device_properties(device).total_memory / (1024**3)
        used_gb = torch.cuda.memory_allocated(device) / (1024**3)
        print(f"  ğŸ’¾ GPU å†…å­˜: {used_gb:.2f}/{total_gb:.2f} GB")
    else:
        print("  âš ï¸ GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
    
    print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"  å®éªŒåç§°: {experiment_name}")
    print(f"  æ¨¡å‹å®½åº¦: {width} (é€šé“æ•°)")
    print(f"  Dropoutç‡: {dropout_rate}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  å­¦ä¹ ç‡: {learning_rate}")
    print(f"  æƒé‡è¡°å‡: {weight_decay}")
    print(f"  æ¢¯åº¦è£å‰ª: {gradient_clip_norm}")
    print(f"  æ—©åœpatience: {early_stopping_patience}")
    print(f"  LRè°ƒåº¦å™¨: {lr_scheduler_type}")
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    _, final_metrics = train_and_test()

    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ç»ˆæµ‹è¯•ç»“æœ:")
    print(f"  Loss: {final_metrics['loss']:.4f}")
    print(f"  PSNR: {final_metrics['psnr']:.2f} dB")
    print(f"  SSIM: {final_metrics['ssim']:.4f}")


if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    main()