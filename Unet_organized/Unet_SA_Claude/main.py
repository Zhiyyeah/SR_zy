import os
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import numpy as np
import time
import json
import matplotlib.pyplot as plt

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from model_attention import UNetSA
from data_loader import create_train_test_dataloaders
from metrics import psnr, compute_ssim
from utils import visualize_results, save_plots, get_device

# ====================== é…ç½®å‚æ•° ======================
class Config:
    """è®­ç»ƒé…ç½®ç±»"""
    # å®éªŒè®¾ç½®
    experiment_name = 'zy_computer_1'
    
    # æ•°æ®è·¯å¾„
    lr_dir = 'SR_zy/Imagey/Imagery_WaterLand/WaterLand_TOA_tiles_lr'
    hr_dir = 'SR_zy/Imagey/Imagery_WaterLand/WaterLand_TOA_tiles_hr'
    train_ratio = 0.8
    
    # æ¨¡å‹å‚æ•°
    up_scale = 8
    width = 32
    dropout_rate = 0.1
    num_channels = 7
    
    # è®­ç»ƒå‚æ•°
    batch_size = 8
    epochs = 200
    learning_rate = 1e-4
    weight_decay = 1e-5
    
    # ç³»ç»Ÿè®¾ç½®
    num_workers = 4
    pin_memory = True
    seed = 42
    device = get_device()
    
    # è¾“å‡ºè®¾ç½®
    output_dir = './outputs'
    save_interval = 1
    
    # å¯è§†åŒ–è®¾ç½®
    rgb_channels = [3, 2, 1]


# ====================== è®­ç»ƒå‡½æ•° ======================
def train_one_epoch(model, dataloader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    
    # åˆå§‹åŒ–æŒ‡æ ‡
    total_loss = 0.0
    total_psnr = 0.0
    total_ssim = 0.0
    num_batches = len(dataloader)
    
    # è¿›åº¦æ¡
    progress_bar = tqdm(dataloader, desc="è®­ç»ƒä¸­", ncols=100)
    
    for batch_idx, (lr_imgs, hr_imgs) in enumerate(progress_bar):
        # æ•°æ®ç§»åˆ°GPU
        lr_imgs = lr_imgs.to(device)
        hr_imgs = hr_imgs.to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        sr_imgs = model(lr_imgs)
        loss = criterion(sr_imgs, hr_imgs)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # è®¡ç®—æŒ‡æ ‡
        batch_psnr = psnr(hr_imgs, sr_imgs)
        batch_ssim = compute_ssim(hr_imgs, sr_imgs)
        
        # ç´¯ç§¯æŒ‡æ ‡
        total_loss += loss.item()
        total_psnr += batch_psnr
        total_ssim += batch_ssim
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'psnr': f'{batch_psnr:.2f}',
            'ssim': f'{batch_ssim:.4f}'
        })
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_metrics = {
        'loss': total_loss / num_batches,
        'psnr': total_psnr / num_batches,
        'ssim': total_ssim / num_batches
    }
    
    return avg_metrics


def evaluate_model(model, dataloader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    
    # åˆå§‹åŒ–æŒ‡æ ‡
    total_loss = 0.0
    total_psnr = 0.0
    total_ssim = 0.0
    num_batches = len(dataloader)
    
    # è¿›åº¦æ¡
    progress_bar = tqdm(dataloader, desc="æµ‹è¯•ä¸­", ncols=100)
    
    with torch.no_grad():
        for lr_imgs, hr_imgs in progress_bar:
            # æ•°æ®ç§»åˆ°GPU
            lr_imgs = lr_imgs.to(device)
            hr_imgs = hr_imgs.to(device)
            
            # å‰å‘ä¼ æ’­
            sr_imgs = model(lr_imgs)
            loss = criterion(sr_imgs, hr_imgs)
            
            # è®¡ç®—æŒ‡æ ‡
            batch_psnr = psnr(hr_imgs, sr_imgs)
            batch_ssim = compute_ssim(hr_imgs, sr_imgs)
            
            # ç´¯ç§¯æŒ‡æ ‡
            total_loss += loss.item()
            total_psnr += batch_psnr
            total_ssim += batch_ssim
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'psnr': f'{batch_psnr:.2f}',
                'ssim': f'{batch_ssim:.4f}'
            })
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_metrics = {
        'loss': total_loss / num_batches,
        'psnr': total_psnr / num_batches,
        'ssim': total_ssim / num_batches
    }
    
    return avg_metrics


def save_checkpoint(model, optimizer, epoch, metrics, save_path):
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics
    }
    torch.save(checkpoint, save_path)


def create_loss_function():
    """åˆ›å»ºæ··åˆæŸå¤±å‡½æ•°"""
    l1_criterion = nn.L1Loss()
    mse_criterion = nn.MSELoss()
    
    def mixed_loss(pred, target):
        return 0.7 * l1_criterion(pred, target) + 0.3 * mse_criterion(pred, target)
    
    return mixed_loss


def print_training_info(config):
    """æ‰“å°è®­ç»ƒä¿¡æ¯"""
    print("="*60)
    print("è¶…åˆ†è¾¨ç‡æ¨¡å‹è®­ç»ƒ")
    print("="*60)
    
    print("\nğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"  â€¢ å®éªŒåç§°: {config.experiment_name}")
    print(f"  â€¢ ä¸Šé‡‡æ ·å€æ•°: {config.up_scale}x")
    print(f"  â€¢ æ¨¡å‹å®½åº¦: {config.width}")
    print(f"  â€¢ æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  â€¢ è®­ç»ƒè½®æ•°: {config.epochs}")
    print(f"  â€¢ å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"  â€¢ æƒé‡è¡°å‡: {config.weight_decay}")
    
    print("\nğŸ’» ç³»ç»Ÿä¿¡æ¯:")
    if torch.cuda.is_available():
        print(f"  â€¢ ä½¿ç”¨è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        device_props = torch.cuda.get_device_properties(0)
        print(f"  â€¢ GPUå†…å­˜: {device_props.total_memory / 1024**3:.1f} GB")
    else:
        print("  â€¢ ä½¿ç”¨è®¾å¤‡: CPU")


# ====================== ä¸»è®­ç»ƒå‡½æ•° ======================
def train():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # åŠ è½½é…ç½®
    config = Config()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(config.seed)
    np.random.seed(config.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config.seed)
    
    # æ‰“å°è®­ç»ƒä¿¡æ¯
    print_training_info(config)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    experiment_dir = os.path.join(config.output_dir, config.experiment_name)
    model_dir = os.path.join(experiment_dir, 'models')
    plots_dir = os.path.join(experiment_dir, 'plots')
    
    for dir_path in [experiment_dir, model_dir, plots_dir]:
        os.makedirs(dir_path, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    config_dict = {k: v for k, v in vars(config).items() if not k.startswith('_')}
    with open(os.path.join(experiment_dir, 'config.json'), 'w') as f:
        json.dump(config_dict, f, indent=4, default=str)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“ åŠ è½½æ•°æ®...")
    train_loader, test_loader = create_train_test_dataloaders(
        config.lr_dir, 
        config.hr_dir, 
        config.batch_size, 
        config.train_ratio, 
        config.seed, 
        config.num_workers, 
        config.pin_memory
    )
    print(f"  â€¢ è®­ç»ƒé›†: {len(train_loader.dataset)} å¼ å›¾åƒ")
    print(f"  â€¢ æµ‹è¯•é›†: {len(test_loader.dataset)} å¼ å›¾åƒ")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
    model = UNetSA(
        up_scale=config.up_scale,
        img_channel=config.num_channels,
        width=config.width,
        dropout_rate=config.dropout_rate,
        use_attention=True
    ).to(config.device)
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"  â€¢ æ¨¡å‹å‚æ•°é‡: {num_params:,}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=config.learning_rate, 
        weight_decay=config.weight_decay
    )
    criterion = create_loss_function()
    
    # è®­ç»ƒå†å²è®°å½•
    train_history = {'loss': [], 'psnr': [], 'ssim': []}
    test_history = {'loss': [], 'psnr': [], 'ssim': []}
    best_test_psnr = 0.0
    best_epoch = 0
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    for epoch in range(1, config.epochs + 1):
        print(f"\nè½®æ¬¡ {epoch}/{config.epochs}")
        
        # è®­ç»ƒä¸€ä¸ªepoch
        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, config.device)
        print(f"è®­ç»ƒ - Loss: {train_metrics['loss']:.4f}, "
              f"PSNR: {train_metrics['psnr']:.2f} dB, "
              f"SSIM: {train_metrics['ssim']:.4f}")
        
        # è¯„ä¼°æ¨¡å‹
        test_metrics = evaluate_model(model, test_loader, criterion, config.device)
        print(f"æµ‹è¯• - Loss: {test_metrics['loss']:.4f}, "
              f"PSNR: {test_metrics['psnr']:.2f} dB, "
              f"SSIM: {test_metrics['ssim']:.4f}")
        
        # è®°å½•å†å²
        for key in ['loss', 'psnr', 'ssim']:
            train_history[key].append(train_metrics[key])
            test_history[key].append(test_metrics[key])
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_metrics['psnr'] > best_test_psnr:
            best_test_psnr = test_metrics['psnr']
            best_epoch = epoch
            save_checkpoint(
                model, optimizer, epoch, test_metrics,
                os.path.join(model_dir, 'best_model.pth')
            )
            print(f"âœ¨ ä¿å­˜æœ€ä½³æ¨¡å‹ (PSNR: {best_test_psnr:.2f} dB)")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % config.save_interval == 0:
            save_checkpoint(
                model, optimizer, epoch, test_metrics,
                os.path.join(model_dir, f'checkpoint_epoch_{epoch}.pth')
            )
            
            # ä¿å­˜è®­ç»ƒæ›²çº¿
            save_plots(train_history, test_history, plots_dir)
    
    # è®­ç»ƒå®Œæˆ
    end_time = time.time()
    training_time = end_time - start_time
    hours = int(training_time // 3600)
    minutes = int((training_time % 3600) // 60)
    seconds = training_time % 60
    
    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"  â€¢ æ€»è€—æ—¶: {hours}å°æ—¶ {minutes}åˆ†é’Ÿ {seconds:.0f}ç§’")
    print(f"  â€¢ æœ€ä½³æ¨¡å‹: ç¬¬ {best_epoch} è½® (PSNR: {best_test_psnr:.2f} dB)")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_checkpoint(
        model, optimizer, config.epochs, test_metrics,
        os.path.join(model_dir, 'final_model.pth')
    )
    
    # ä¿å­˜è®­ç»ƒæ€»ç»“
    summary = {
        'total_epochs': config.epochs,
        'best_epoch': best_epoch,
        'best_test_psnr': best_test_psnr,
        'final_test_metrics': test_metrics,
        'training_time_seconds': training_time,
        'model_parameters': num_params
    }
    
    with open(os.path.join(experiment_dir, 'training_summary.json'), 'w') as f:
        json.dump(summary, f, indent=4)
    
    return model, test_metrics


# ====================== ç¨‹åºå…¥å£ ======================
if __name__ == "__main__":
    model, final_metrics = train()