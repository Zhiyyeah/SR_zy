import torch
import torch.nn as nn
import torch.nn.functional as F


# -----------------------------
# åŸºç¡€æ¨¡å—
# -----------------------------
class SEBlock(nn.Module):
    """Squeeze-and-Excitation é€šé“æ³¨æ„åŠ?""
    def __init__(self, ch: int, reduction: int = 8):
        super().__init__()
        mid = max(1, ch // reduction)
        self.avg = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Conv2d(ch, mid, 1, bias=True),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid, ch, 1, bias=True),
            nn.Sigmoid()
        )
    def forward(self, x):
        w = self.fc(self.avg(x))
        return x * w


class SpatialAttention(nn.Module):
    """CBAM é£æ ¼çš„ç©ºé—´æ³¨æ„åŠ›ï¼ˆä¸ä½ åŸæ¨¡å‹ä¸€è‡´ï¼‰"""
    def __init__(self, kernel_size: int = 7):
        super().__init__()
        p = (kernel_size - 1) // 2
        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=p, bias=False)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        attn = torch.cat([avg_out, max_out], dim=1)
        attn = self.sigmoid(self.conv(attn))
        return x * attn


def _best_gn_groups(ch: int, preferred: int = 8) -> int:
    g = min(preferred, ch)
    while g > 1 and (ch % g != 0):
        g -= 1
    return max(1, g)


class DSConv7x7(nn.Module):
    """Depthwise-Separable 7x7ï¼ˆå…ˆDWå†PWï¼‰ç”¨äºå¹³æ»‘ä½é¢?""
    def __init__(self, ch: int):
        super().__init__()
        self.dw = nn.Conv2d(ch, ch, 7, padding=3, groups=ch, bias=False)
        self.pw = nn.Conv2d(ch, ch, 1, bias=False)
    def forward(self, x):
        return self.pw(self.dw(x))


# -----------------------------
# æ®‹å·®å—ï¼ˆæ›´ç¨³çš?dilationï¼ŒåŠ å…?SE ï¼?# -----------------------------
class SASEBlock(nn.Module):
    """
    SpatialAttention + (Conv3x3(dilation)->GN->ReLU->Conv3x3(dilation)->GN) + æ®‹å·®
    """
    def __init__(self, ch: int, dilation: int = 1, groups: int = 8, dropout: float = 0.0):
        super().__init__()
        g = _best_gn_groups(ch, groups)
        pad = dilation
        self.sa = SpatialAttention()
        self.conv1 = nn.Conv2d(ch, ch, 3, padding=pad, dilation=dilation, bias=False)
        self.gn1 = nn.GroupNorm(g, ch)
        self.act1 = nn.ReLU(inplace=True)
        self.drop = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()
        self.conv2 = nn.Conv2d(ch, ch, 3, padding=pad, dilation=dilation, bias=False)
        self.gn2 = nn.GroupNorm(g, ch)
        self.acto = nn.ReLU(inplace=True)

    def forward(self, x):
        identity = x
        x = self.sa(x)
        x = self.conv1(x)
        x = self.gn1(x)
        x = self.act1(x)
        x = self.drop(x)
        x = self.conv2(x)
        x = self.gn2(x)
        return self.acto(x + identity)


# -----------------------------
# ä½é¢‘é‡‘å­—å¡”ï¼ˆå¤šå°ºåº¦å¹³å‡æ± åŒ?+ ä¸Šé‡‡æ ?+ å¹³æ»‘ï¼?# -----------------------------
class LowFreqPyramid(nn.Module):
    """
    å¤šå°ºåº?AvgPool -> Conv3x3 -> ReLU -> ä¸Šé‡‡æ ·å›åŸå°ºå¯¸ï¼Œèšåˆåç”¨ 7x7 æ·±åº¦å¯åˆ†ç¦»å·ç§¯å¹³æ»?    """
    def __init__(self, ch: int, scales=(2, 4, 8)):
        super().__init__()
        self.scales = scales
        self.branches = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(ch, ch, 3, padding=1, bias=True),
                nn.ReLU(inplace=True)
            ) for _ in scales
        ])
        

    def forward(self, x):
        H, W = x.shape[-2], x.shape[-1]
        outs = []
        for i, s in enumerate(self.scales):
            # å¹³å‡æ± åŒ–è·å–ä½é¢‘
            pooled = F.avg_pool2d(x, kernel_size=s, stride=s, ceil_mode=True)
