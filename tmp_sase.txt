    with torch.no_grad():
        y2 = model2(x2)
    print("Scale=2:", tuple(x2.shape), "->", tuple(y2.shape), "Params:", f"{count_parameters(model2):,}")
import torch
import torch.nn as nn
import torch.nn.functional as F


# -----------------------------
# åŸºç¡€æ¨¡å—
# -----------------------------
class SEBlock(nn.Module):
    """Squeeze-and-Excitation é€šé“æ³¨æ„åŠ?""
    def __init__(self, ch: int, reduction: int = 8):
        super().__init__()
        mid = max(1, ch // reduction)
        self.avg = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Conv2d(ch, mid, 1, bias=True),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid, ch, 1, bias=True),
            nn.Sigmoid()
        )
    def forward(self, x):
        w = self.fc(self.avg(x))
        return x * w


class SpatialAttention(nn.Module):
    """CBAM é£æ ¼çš„ç©ºé—´æ³¨æ„åŠ›ï¼ˆä¸ä½ åŸæ¨¡å‹ä¸€è‡´ï¼‰"""
    def __init__(self, kernel_size: int = 7):
        super().__init__()
        p = (kernel_size - 1) // 2
        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=p, bias=False)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        attn = torch.cat([avg_out, max_out], dim=1)
        attn = self.sigmoid(self.conv(attn))
        return x * attn


def _best_gn_groups(ch: int, preferred: int = 8) -> int:
    g = min(preferred, ch)
    while g > 1 and (ch % g != 0):
        g -= 1
    return max(1, g)


class DSConv7x7(nn.Module):
    """Depthwise-Separable 7x7ï¼ˆå…ˆDWå†PWï¼‰ç”¨äºå¹³æ»‘ä½é¢?""
    def __init__(self, ch: int):
        super().__init__()
        self.dw = nn.Conv2d(ch, ch, 7, padding=3, groups=ch, bias=False)
        self.pw = nn.Conv2d(ch, ch, 1, bias=False)
    def forward(self, x):
        return self.pw(self.dw(x))


# -----------------------------
